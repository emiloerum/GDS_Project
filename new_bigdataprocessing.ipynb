{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emilo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra cleaning functions for dates and \"|\" symbol\n",
    "def remove_dates_from_content(content):\n",
    "    '''Function that attempts to substitute dates in a document for the token \"_DATE_\".\n",
    "    If it fails to do so - for example if the content is not convertable to string, it \n",
    "    handles the typeerror exception and doesnt do anything with the content.'''\n",
    "    date_pattern = re.compile(r\"(([0-9]{4}-(0[0-9]|1[0-2])-([0-2][0-9]|[3[01])|[a-z]{,9} [0-9]{1,2}, [0-9]{2,4})|\\b(\\w+\\s)(\\d{2})(th)?,?(\\s\\d{4})\\b)\")\n",
    "    try:\n",
    "        content_without_dates = re.sub(date_pattern, \"_DATE_\", str(content))\n",
    "    except TypeError:\n",
    "        content_without_dates = content\n",
    "    return content_without_dates \n",
    "\n",
    "def remove_bar_from_content(content):\n",
    "    '''Function for removing every occurence of \"|\"'''\n",
    "    content_without_bar = str(content).replace(\"|\", \"\")\n",
    "    return content_without_bar\n",
    "\n",
    "def remove_a_from_content(content):\n",
    "    '''Function for removing every occurence of \"a\"'''\n",
    "    return [word for word in content if word != \"a\"]\n",
    "\n",
    "#Reading in stopwords\n",
    "stopwordsDF = pd.read_csv('stopwords.csv')\n",
    "stopwordsSeries = stopwordsDF.squeeze()\n",
    "stopwords = set(stopwordsSeries)\n",
    "\n",
    "def remove_stopwords(list):\n",
    "     '''Function that returns a list containing a document with the stopwords removed'''\n",
    "     return [word for word in list if word not in stopwords]\n",
    "\n",
    "#Initializing stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def list_stemmer (wordlist): #stemmer hvert ord i en liste\n",
    "    '''Function that stems each word in the given input list and returns this'''\n",
    "    stemmed_list = []\n",
    "    for word in wordlist:\n",
    "        stemmed_list.append(stemmer.stem(word))\n",
    "    return stemmed_list\n",
    "\n",
    "def vocabulary_size(series):\n",
    "    '''Function that computes the size of the vocabulary of a corpus'''\n",
    "\n",
    "    #Initializing empty set to store unique words\n",
    "    unique_words_in_corpus = set()\n",
    "\n",
    "    for lst in series:\n",
    "        #Updates the set of unique words by union with all lists in the corpus\n",
    "        unique_words_in_corpus.update(lst)\n",
    "    \n",
    "    #Computing length (sz of vocabulary)\n",
    "    vocab_sz = len(unique_words_in_corpus)\n",
    "\n",
    "    return vocab_sz\n",
    "\n",
    "def most_frequent_n_words(series, n):\n",
    "    '''Function that return the n most frequent words and their frequencies in a series\n",
    "    assuming the elements in the series are lists of strings'''\n",
    "    words = (word for sublist in series for word in sublist)\n",
    "    \n",
    "    # Calculate word frequencies using Counter\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Select the top 10,000 most common words and frequencies\n",
    "    most_common_words = word_freq.most_common(n)\n",
    "\n",
    "    return most_common_words\n",
    "\n",
    "#Jeg har taget foreskellen ml antal ord før processing og efter of så divideret med\n",
    "# antal ord til at starte med for at få reduction raten\n",
    "def reduction_rate(after,before):\n",
    "     '''Computes the reduction rate of the size of the vocabulary\n",
    "     and returns this rounded to 3 decimal points'''\n",
    "     return round((before - after)/before, 3)\n",
    "#måske noget med at ordene er meget frekvente, så man kunne også kigge på reduktionen af antal ord.\n",
    "\n",
    "\n",
    "# def word_frequency_plot(series, title):\n",
    "#     '''function for plotting from the third most frequent word to the 1000 most frequent word\n",
    "#     as well as their corresponding frequencies in a barplot.'''\n",
    "#     plt.bar(*zip(*most_frequent(series, 0, 9999).items()))\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('words')\n",
    "#     plt.ylabel('frequency')\n",
    "#     plt.xticks([])\n",
    "#     plt.ylim(0, 26149872)\n",
    "#     plt.show()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def word_frequency_plot(series, title):\n",
    "    \"\"\"\n",
    "    Plots the frequency of the 10,000 most frequent words in the series without x-ticks.\n",
    "    Args:\n",
    "    - series: A Pandas Series where each element is a list of words from a document.\n",
    "    - title: Title for the plot.\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of words using a generator expression\n",
    "    words = (word for sublist in series for word in sublist)\n",
    "    \n",
    "    # Calculate word frequencies using Counter\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Select the top 10,000 most common words and frequencies\n",
    "    most_common_words = word_freq.most_common(10000)\n",
    "    words, frequencies = zip(*most_common_words)  # Unzipping the word-frequencies pairs\n",
    "    \n",
    "    # Creating the plot\n",
    "    plt.figure(figsize=(20, 10))  # Increase figure size for better visibility\n",
    "    plt.bar(range(len(frequencies)), frequencies, width=1.0)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Words Ranked by Frequency')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.yscale('log')  # Using logarithmic scale for better visibility of frequencies\n",
    "    \n",
    "    # Removing x-ticks\n",
    "    plt.xticks([])\n",
    "    \n",
    "    plt.tight_layout()  # Adjust layout to not cut off labels\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage (commented out):\n",
    "# word_frequency_plot_vast(content_series, \"Top 10,000 Most Frequent Words\")\n",
    "\n",
    "\n",
    "def count_occurences_in_content(content, str):\n",
    "    count = sum(string == str for document in content for string in document)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying data preprocessing pipeline to 995,000_rows.csv dataset split into chunks for better memory management\n",
    "\n",
    "# chunk_size = 248750\n",
    "# large_dataset_chunks = pd.read_csv('995,000_rows.csv', low_memory=False, chunksize=chunk_size)\n",
    "\n",
    "# #Initializing list to save the processed data\n",
    "# preprocessed_content_list = []\n",
    "# vocabulary_size_with_stopwords = 0\n",
    "# vocabulary_size_no_stopwords = 0\n",
    "\n",
    "# for chunk in large_dataset_chunks:\n",
    "\n",
    "#     #Dropping empty columns\n",
    "#     chunk = chunk.drop(columns=['keywords', 'summary'])\n",
    "\n",
    "#     #Extracting 'content' column chunks and giving dtype string\n",
    "#     chunk_content_column = chunk['content'].astype(str)\n",
    "\n",
    "#     #Cleaning the 'content' column chunks\n",
    "#     chunk_content_column = chunk_content_column.apply(lambda x : clean(x,\n",
    "#     fix_unicode=False,             # fix various unicode errors\n",
    "#     to_ascii=False,                # transliterate to closest ASCII representation\n",
    "#     lower=True,                    # lowercase text\n",
    "#     no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "#     no_urls=True,                  # replace all URLs with a special token\n",
    "#     no_emails=True,                # replace all email addresses with a special token\n",
    "#     no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "#     no_numbers=True,               # replace all numbers with a special token\n",
    "#     no_digits=True,                # replace all digits with a special token\n",
    "#     no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "#     no_punct=True,                 # remove punctuations\n",
    "#     replace_with_punct=\"\",         # instead of removing punctuations you may replace them\n",
    "#     replace_with_url=\"_URL_\",\n",
    "#     replace_with_email=\"_EMAIL_\",\n",
    "#     replace_with_phone_number=\"_PHONE_\",\n",
    "#     replace_with_number=\"_NUMBER_\",\n",
    "#     replace_with_digit=\"0\",\n",
    "#     replace_with_currency_symbol=\"_CUR_\",\n",
    "#     lang=\"en\"                    \n",
    "# ))\n",
    "#     #Final cleaning of the 'content' column chunks\n",
    "#     chunk_content_column = chunk_content_column.apply(lambda x: remove_dates_from_content(x))\n",
    "#     chunk_content_column = chunk_content_column.apply(lambda x: remove_bar_from_content(x))\n",
    "#     print('Cleaning finished')\n",
    "\n",
    "#     #Tokenizing the 'content' column chunks\n",
    "#     chunk_content_column = chunk_content_column.apply(lambda x: nltk.word_tokenize(x))\n",
    "#     print('Tokenization finished')\n",
    "\n",
    "#     #Counting vocabulary size before stopwords removal\n",
    "#     vocabulary_size_with_stopwords += vocabulary_size(chunk_content_column)[0]\n",
    "\n",
    "#     #Removing stopwords from the 'content' column chunks\n",
    "#     chunk_content_column = chunk_content_column.apply(lambda x: remove_stopwords(x))\n",
    "    \n",
    "#     #Counting vocabulary size after stopwords removal\n",
    "#     vocabulary_size_no_stopwords += vocabulary_size(chunk_content_column)[0]\n",
    "\n",
    "#     #Stemming the 'content' column chunks\n",
    "#     chunk_content_column = chunk_content_column.apply(lambda x:list_stemmer(x))\n",
    "#     print('Stemming finished')\n",
    "#     preprocessed_content_list.extend(chunk_content_column.tolist())\n",
    "\n",
    "# print('Preprocessing finished')\n",
    "\n",
    "# print(\"vocabulary size before removal of stopwords: \", vocabulary_size_with_stopwords)\n",
    "# print(\"vocabulary size after removal of stopwords: \", vocabulary_size_no_stopwords)\n",
    "\n",
    "# print(\"Reduction rate of vocabulary size after removing stopwords:\", reduction_rate(vocabulary_size_no_stopwords,vocabulary_size_with_stopwords))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_column = pd.read_csv('995,000_rows.csv', low_memory=False, usecols=['content'], dtype={'content': str})['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the 'content' column chunks\n",
    "content_column = content_column.apply(lambda x : clean(x,\n",
    "fix_unicode=False,             # fix various unicode errors\n",
    "to_ascii=False,                # transliterate to closest ASCII representation\n",
    "lower=True,                    # lowercase text\n",
    "no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "no_urls=True,                  # replace all URLs with a special token\n",
    "no_emails=True,                # replace all email addresses with a special token\n",
    "no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "no_numbers=True,               # replace all numbers with a special token\n",
    "no_digits=True,                # replace all digits with a special token\n",
    "no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "no_punct=True,                 # remove punctuations\n",
    "replace_with_punct=\"\",         # instead of removing punctuations you may replace them\n",
    "replace_with_url=\"_URL_\",\n",
    "replace_with_email=\"_EMAIL_\",\n",
    "replace_with_phone_number=\"_PHONE_\",\n",
    "replace_with_number=\"_NUMBER_\",\n",
    "replace_with_digit=\"0\",\n",
    "replace_with_currency_symbol=\"_CUR_\",\n",
    "lang=\"en\"                    \n",
    "))\n",
    "\n",
    "#Final cleaning of the 'content' column\n",
    "content_column = content_column.apply(lambda x: remove_dates_from_content(x))\n",
    "content_column = content_column.apply(lambda x: remove_bar_from_content(x))\n",
    "print('Cleaning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the 'content' column chunks\n",
    "content_column = content_column.apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting stuff\n",
    "print(\"Count of URL's in content: \", count_occurences_in_content(content_column, \"url\"))\n",
    "print(\"Count of dates in content: \", count_occurences_in_content(content_column, \"date\"))\n",
    "print(\"Count of numbers in content: \", count_occurences_in_content(content_column, \"number\"))\n",
    "\n",
    "#Counting vocabulary size before stopwords removal\n",
    "vocabulary_size_with_stopwords = vocabulary_size(content_column)\n",
    "print(\"vocabulary size before removing stopwords: \", vocabulary_size_with_stopwords)\n",
    "\n",
    "#100 most frequent words before removing stopwords\n",
    "most_frequent_with_stopwords = most_frequent_n_words(content_column, 10000)\n",
    "print(\"100 most frequent words before removing stopwords\", most_frequent_with_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the frequency of the 10000 most frequent words before removing stopwords\n",
    "word_frequency_plot(content_column, \"With stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords from the 'content' column chunks\n",
    "content_column = content_column.apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting vocabulary size after stopwords removal\n",
    "vocabulary_size_no_stopwords = vocabulary_size(content_column)\n",
    "print(\"vocabulary size before removing stopwords: \", vocabulary_size_no_stopwords)\n",
    "\n",
    "#100 most frequent words before removing stopwords\n",
    "most_frequent_no_stopwords = most_frequent_n_words(content_column, 100)\n",
    "print(\"100 most frequent words before removing stopwords\", most_frequent_no_stopwords)\n",
    "\n",
    "#Vocabulary reduction after removing stopwords\n",
    "print(\"Reduction rate of vocabulary size after removing stopwords:\", reduction_rate(vocabulary_size_no_stopwords, vocabulary_size_with_stopwords))\n",
    "\n",
    "#Plotting the frequency of the 10000 most frequent words before removing stopwords\n",
    "word_frequency_plot(content_column, \"No stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming the 'content' column chunks\n",
    "content_column = content_column.apply(lambda x:list_stemmer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting vocabulary size after stemming\n",
    "vocabulary_size_stemmed = vocabulary_size(content_column)\n",
    "print(\"vocabulary size after stemming: \", vocabulary_size_stemmed)\n",
    "\n",
    "#100 most frequent words after stemming\n",
    "most_frequent_stemmed = most_frequent_n_words(content_column, 100)\n",
    "print(\"most frequent stemmed\", most_frequent_stemmed)\n",
    "\n",
    "#Vocabulary reduction after stemming\n",
    "print(\"Reduction rate of vocabulary size after removing stopwords:\", reduction_rate(vocabulary_size_stemmed, vocabulary_size_no_stopwords))\n",
    "\n",
    "word_frequency_plot(content_column, \"Stemmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset = pd.read_csv(\"995,000_rows.csv\", low_memory=False)\n",
    "\n",
    "#Dropping empty columns\n",
    "large_dataset = large_dataset.drop(columns=['keywords', 'summary'], inplace=True)\n",
    "\n",
    "#Altering the 'content' to contain the preprocessed content\n",
    "large_dataset['content'] = content_column\n",
    "\n",
    "#Saving preprocessed dataframe to new csv file\n",
    "large_dataset.to_pickle('large_dataset_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #None of the following code has been run yet!!\n",
    "\n",
    "# large_dataset_cleaned = pd.read_csv('large_dataset_cleaned.csv')\n",
    "# #Splitting data into train_test_val sets:\n",
    "\n",
    "# #Dividing data into features and label (X and y)\n",
    "# y = large_dataset_cleaned['type'] #target\n",
    "\n",
    "# X = large_dataset_cleaned.drop(columns=['type']) #features\n",
    "# data = pd.read_csv('large_dataset_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split data into 80% training and 20% test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# #Split the newly created test data equally into validation and test data (10% each of the total dataset)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
