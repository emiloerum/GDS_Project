{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\astri\\AppData\\Local\\Temp\\ipykernel_5540\\1349032984.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m literal_eval\n\u001b[1;32m---> 11\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbig_clean_data_pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munreliable\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\astri\\miniconda3\\Lib\\site-packages\\pandas\\io\\pickle.py:202\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;66;03m# We want to silence any warnings about, e.g. moved modules.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[1;32m--> 202\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, nltk, collections\n",
    "import pandas as pd\n",
    "import collections\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "data = pd.read_pickle(\"big_clean_data_pickle\")\n",
    "data = data[data['type'] != \"unreliable\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_target(x):\n",
    "    if x==\"reliable\":\n",
    "        return 0\n",
    "    if x==\"fake\":\n",
    "        return 1\n",
    "\n",
    "X = data.drop([\"type\"],axis=1).drop([\"simple_type\"],axis=1) #lav x-vector\n",
    "y = data[\"simple_type\"].apply(lambda x:bin_target(x))#lav target-vector\n",
    "\n",
    "X_train, X_val, y_train, y_val= train_test_split(X,y,test_size=0.2,random_state=0,shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val,y_val, test_size=0.5,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827974\n"
     ]
    }
   ],
   "source": [
    "#get vocabulary from X_train\n",
    "def vocabulary_size(series):\n",
    "    '''Computes the vocabulary size of a corpus by iterating through every word\n",
    "    in every document in the corpus and adding it to a dictionary if it hasnt been seen before\n",
    "    as well as incrementing a variable counting every unique word. Returns the unique_word_count\n",
    "    and sorted list of words and their frequencies'''\n",
    "    # Initialize a counter for unique words count and dictionary to store word frequencies\n",
    "    unique_words_count = 0\n",
    "    word_frequency = {}\n",
    "    # Iterate through each element in the series\n",
    "    for field in series:\n",
    "        # Iterate through each word in the current element\n",
    "        for word in field:\n",
    "            if word in word_frequency: \n",
    "                # If word already in dict, increment its count\n",
    "                word_frequency[word] += 1\n",
    "            else:\n",
    "                # If not add it to the dictionary with count 1 and increment unique_words_count\n",
    "                word_frequency[word] = 1\n",
    "                unique_words_count += 1\n",
    "    # Sort the word_frequency dictionary by frequency in descending order\n",
    "    return word_frequency\n",
    "\n",
    "#vocabulary is key in dict^\n",
    "vocabulary = [*vocabulary_size(X_train[\"content\"])]\n",
    "\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m      4\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: Counter([y \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m vocabulary])\n\u001b[1;32m----> 5\u001b[0m X_BoW_train \u001b[38;5;241m=\u001b[39m (pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(f)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m      6\u001b[0m                \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m                \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      8\u001b[0m                \u001b[38;5;241m.\u001b[39mreindex(columns\u001b[38;5;241m=\u001b[39mvocabulary))\n\u001b[0;32m     10\u001b[0m X_BoW_val \u001b[38;5;241m=\u001b[39m (pd\u001b[38;5;241m.\u001b[39mDataFrame(X_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(f)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     11\u001b[0m                \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     12\u001b[0m                \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     13\u001b[0m                \u001b[38;5;241m.\u001b[39mreindex(columns\u001b[38;5;241m=\u001b[39mvocabulary))\n",
      "File \u001b[1;32mc:\\Users\\astri\\miniconda3\\Lib\\site-packages\\pandas\\core\\series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4780\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4898\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4899\u001b[0m         func,\n\u001b[0;32m   4900\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4901\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4902\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4903\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4904\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\Users\\astri\\miniconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\astri\\miniconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\astri\\miniconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\Users\\astri\\miniconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#making a dataset with BoW representation of stemmed articles\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m----> 4\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: Counter([y \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m vocabulary])\n\u001b[0;32m      5\u001b[0m X_BoW_train \u001b[38;5;241m=\u001b[39m (pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(f)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m      6\u001b[0m                \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m                \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      8\u001b[0m                \u001b[38;5;241m.\u001b[39mreindex(columns\u001b[38;5;241m=\u001b[39mvocabulary))\n\u001b[0;32m     10\u001b[0m X_BoW_val \u001b[38;5;241m=\u001b[39m (pd\u001b[38;5;241m.\u001b[39mDataFrame(X_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(f)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     11\u001b[0m                \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     12\u001b[0m                \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     13\u001b[0m                \u001b[38;5;241m.\u001b[39mreindex(columns\u001b[38;5;241m=\u001b[39mvocabulary))\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#making a dataset with BoW representation of stemmed articles\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m----> 4\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: Counter([y \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m vocabulary])\n\u001b[0;32m      5\u001b[0m X_BoW_train \u001b[38;5;241m=\u001b[39m (pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(f)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m      6\u001b[0m                \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m                \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      8\u001b[0m                \u001b[38;5;241m.\u001b[39mreindex(columns\u001b[38;5;241m=\u001b[39mvocabulary))\n\u001b[0;32m     10\u001b[0m X_BoW_val \u001b[38;5;241m=\u001b[39m (pd\u001b[38;5;241m.\u001b[39mDataFrame(X_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(f)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     11\u001b[0m                \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     12\u001b[0m                \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     13\u001b[0m                \u001b[38;5;241m.\u001b[39mreindex(columns\u001b[38;5;241m=\u001b[39mvocabulary))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#making a dataset with BoW representation of stemmed articles\n",
    "from collections import Counter\n",
    "\n",
    "f = lambda x: Counter([y for y in x if y in vocabulary])\n",
    "X_BoW_train = (pd.DataFrame(X_train['content'].apply(f).values.tolist())\n",
    "               .fillna(0)\n",
    "               .astype(int)\n",
    "               .reindex(columns=vocabulary))\n",
    "\n",
    "X_BoW_val = (pd.DataFrame(X_val['content'].apply(f).values.tolist())\n",
    "               .fillna(0)\n",
    "               .astype(int)\n",
    "               .reindex(columns=vocabulary))\n",
    "#tæller det som leakage at man bruger vocabulary fra hele datasættet??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_BoW_train.head())\n",
    "print(X_BoW_val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_BoW_train.to_csv(\"big_BoW_data_train.csv\",index =True)\n",
    "X_BoW_val.to_csv(\"big_BoW_data_val.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_BoW_train, y_train)\n",
    "clf.score(X_BoW_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(doc):\n",
    "   return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=fun,\n",
    "    preprocessor=fun,\n",
    "    token_pattern=None)  \n",
    "\n",
    "#tfidf.fit(data[\"content_stemmed\"]) #vær opmærksom på at vocab nu er relativt til træningssættet, men det skal det jo sådan set også være, ellers ville der være leakage\n",
    "#tfidf.vocabulary_\n",
    "#X_tfidf = data[\"content_stemmed\"].apply(lambda x: tfidf.transform([x]))\n",
    "#X_tfidf= data[\"content_stemmed\"].apply(lambda x: tfidf.transform([x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#neural network with BoW\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "clf.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "156\n",
      "(19, 1754)\n",
      "(156, 1754)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making data ready: tf-idf representation\n",
    "X = data.drop([\"type\"],axis=1).drop([\"simple_type\"],axis=1) #lav x-vector\n",
    "y = data[\"simple_type\"].apply(lambda x:bin_target(x)).astype(int)\n",
    "\n",
    "X_train, X_val, y_train, y_val= train_test_split(X,y,test_size=0.2,random_state=0,shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val,y_val, test_size=0.5,random_state=0,shuffle=True) \n",
    "\n",
    "print(len(X_val))\n",
    "print(len(X_train))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True, max_df=0.5, min_df=5, stop_words=\"english\"\n",
    "    )\n",
    "\n",
    "vectorizer.fit(X_train[\"content\"]) #det fungerer nok ikke for det cleanede datasæt..\n",
    "X_train = vectorizer.transform(X_train[\"content\"])\n",
    "X_test = vectorizer.transform(X_val[\"content\"]) #important that vectorizer is only fittet on training data - otherwise leakage\n",
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "clf.score(X_test,y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alt herunder skal nok ikke bruges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tfIdfTransformer \u001b[38;5;241m=\u001b[39m TfidfTransformer(use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m countVectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m----> 3\u001b[0m X_BoW \u001b[38;5;241m=\u001b[39m countVectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n",
      "File \u001b[1;32mc:\\Users\\astri\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\astri\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\astri\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\astri\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\astri\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#tfIdfTransformer = TfidfTransformer(use_idf=True)\n",
    "#countVectorizer = CountVectorizer()\n",
    "#X_BoW = countVectorizer.fit_transform(X_train)\n",
    "#det virker lidt som om den antager at den ikke får en tokenized tekst?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=fun,\n",
    "    preprocessor=fun,\n",
    "    token_pattern=None)  \n",
    "\n",
    "tfidf.fit(X_train[\"content_stemmed\"]) #vær opmærksom på at vocab nu er relativt til træningssættet, men det skal det jo sådan set også være, ellers ville der være leakage\n",
    "tfidf.vocabulary_\n",
    "X_tfidf = X_train[\"content_stemmed\"].apply(lambda x: tfidf.transform([x]))\n",
    "X_tfidf = X_train[\"content_stemmed\"].apply(lambda x: tfidf.transform([x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71       (0, 9709)\\t0.027082482358835893\\n  (0, 9685)...\n",
      "160      (0, 9638)\\t0.02626083399350609\\n  (0, 9599)\\...\n",
      "180      (0, 9650)\\t0.055103682695849705\\n  (0, 9602)...\n",
      "75       (0, 9607)\\t0.03748492461728866\\n  (0, 9586)\\...\n",
      "59       (0, 9520)\\t0.10499222703737843\\n  (0, 9438)\\...\n",
      "                             ...                        \n",
      "67       (0, 9651)\\t0.0112136559676212\\n  (0, 9622)\\t...\n",
      "192      (0, 9401)\\t0.07658557914355257\\n  (0, 9151)\\...\n",
      "117      (0, 9560)\\t0.03211953463053134\\n  (0, 9504)\\...\n",
      "47       (0, 9674)\\t0.022602832963982845\\n  (0, 9651)...\n",
      "172      (0, 8990)\\t0.27707159300210976\\n  (0, 8673)\\...\n",
      "Name: content_stemmed, Length: 200, dtype: object\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_tfidf, y_train)\n",
    "clf.score(X_val,y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 36)\t11\n",
      "  (0, 114)\t5\n",
      "  (0, 115)\t2\n",
      "  (0, 213)\t1\n",
      "  (0, 283)\t1\n",
      "  (0, 345)\t1\n",
      "  (0, 650)\t1\n",
      "  (0, 762)\t1\n",
      "  (0, 861)\t1\n",
      "  (0, 884)\t1\n",
      "  (0, 1013)\t1\n",
      "  (0, 1088)\t1\n",
      "  (0, 1134)\t1\n",
      "  (0, 1323)\t1\n",
      "  (0, 1431)\t2\n",
      "  (0, 1446)\t4\n",
      "  (0, 1453)\t3\n",
      "  (0, 1662)\t2\n",
      "  (0, 1668)\t1\n",
      "  (0, 1739)\t6\n",
      "  (0, 2002)\t1\n",
      "  (0, 2049)\t1\n",
      "  (0, 2082)\t4\n",
      "  (0, 2120)\t1\n",
      "  (0, 2134)\t1\n",
      "  :\t:\n",
      "  (0, 7999)\t1\n",
      "  (0, 8034)\t1\n",
      "  (0, 8038)\t1\n",
      "  (0, 8077)\t2\n",
      "  (0, 8116)\t1\n",
      "  (0, 8321)\t1\n",
      "  (0, 8330)\t1\n",
      "  (0, 8481)\t1\n",
      "  (0, 8614)\t1\n",
      "  (0, 8673)\t3\n",
      "  (0, 8735)\t1\n",
      "  (0, 8832)\t1\n",
      "  (0, 8990)\t1\n",
      "  (0, 8993)\t2\n",
      "  (0, 9054)\t1\n",
      "  (0, 9302)\t1\n",
      "  (0, 9323)\t1\n",
      "  (0, 9343)\t4\n",
      "  (0, 9363)\t1\n",
      "  (0, 9515)\t1\n",
      "  (0, 9577)\t1\n",
      "  (0, 9584)\t1\n",
      "  (0, 9586)\t4\n",
      "  (0, 9602)\t1\n",
      "  (0, 9651)\t1\n"
     ]
    }
   ],
   "source": [
    "BoW = CountVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=fun,\n",
    "    preprocessor=fun,\n",
    "    token_pattern=None)  \n",
    "\n",
    "BoW.fit(X_train) #vær opmærksom på at vocab nu er relativt til træningssættet, men det skal det jo sådan set også være, ellers ville der være leakage\n",
    "BoW.vocabulary_\n",
    "X_BoW = X_train.apply(lambda x: BoW.transform([x]))\n",
    "\n",
    "\n",
    "print(BoW.transform([X_train[0]]))\n",
    "\n",
    "#tror det er rigtigt??\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
