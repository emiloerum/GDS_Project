{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emilo\\AppData\\Local\\Temp\\ipykernel_2468\\2151088572.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emilo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra cleaning functions for dates and \"|\" symbol\n",
    "def remove_dates_from_content(content):\n",
    "    '''Function that attempts to substitute dates in a document for the token \"_DATE_\".\n",
    "    If it fails to do so - for example if the content is not convertable to string, it \n",
    "    handles the typeerror exception and doesnt do anything with the content.'''\n",
    "    date_pattern = re.compile(r\"(([0-9]{4}-(0[0-9]|1[0-2])-([0-2][0-9]|[3[01])|[a-z]{,9} [0-9]{1,2}, [0-9]{2,4})|\\b(\\w+\\s)(\\d{2})(th)?,?(\\s\\d{4})\\b)\")\n",
    "    try:\n",
    "        content_without_dates = re.sub(date_pattern, \"_DATE_\", str(content))\n",
    "    except TypeError:\n",
    "        content_without_dates = content\n",
    "    return content_without_dates \n",
    "\n",
    "def remove_bar_from_content(content):\n",
    "    '''Function for removing every occurence of \"|\"'''\n",
    "    content_without_bar = str(content).replace(\"|\", \"\")\n",
    "    return content_without_bar\n",
    "\n",
    "#Reading in stopwords\n",
    "stopwordsDF = pd.read_csv('stopwords.csv')\n",
    "stopwordsSeries = stopwordsDF.squeeze()\n",
    "stopwords = set(stopwordsSeries)\n",
    "\n",
    "def remove_stopwords(list):\n",
    "     '''Function that returns a list containing a document with the stopwords removed'''\n",
    "     return [word for word in list if word not in stopwords]\n",
    "\n",
    "#Initializing stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def list_stemmer (wordlist): #stemmer hvert ord i en liste\n",
    "    '''Function that stems each word in the given input list and returns this'''\n",
    "    stemmed_list = []\n",
    "    for word in wordlist:\n",
    "        stemmed_list.append(stemmer.stem(word))\n",
    "    return stemmed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning finished\n",
      "Tokenization finished\n",
      "Stemming finished\n",
      "Cleaning finished\n",
      "Tokenization finished\n",
      "Stemming finished\n",
      "Cleaning finished\n",
      "Tokenization finished\n",
      "Stemming finished\n",
      "Cleaning finished\n",
      "Tokenization finished\n",
      "Stemming finished\n",
      "Preprocessing finished\n"
     ]
    }
   ],
   "source": [
    "#Applying data preprocessing pipeline to 995,000_rows.csv dataset split into chunks for better memory management\n",
    "\n",
    "chunk_size = 248750\n",
    "large_dataset_chunks = pd.read_csv('995,000_rows.csv', low_memory=False, chunksize=chunk_size)\n",
    "\n",
    "#Initializing list to save the processed data\n",
    "preprocessed_content_list = []\n",
    "\n",
    "for chunk in large_dataset_chunks:\n",
    "\n",
    "    #Dropping empty columns\n",
    "    chunk = chunk.drop(columns=['keywords', 'summary'])\n",
    "\n",
    "    #Extracting 'content' column chunks and giving dtype string\n",
    "    chunk_content_column = chunk['content'].astype(str)\n",
    "\n",
    "    #Cleaning the 'content' column chunks\n",
    "    chunk_content_column_cleaned = chunk_content_column.apply(lambda x : clean(x,\n",
    "    fix_unicode=False,             # fix various unicode errors\n",
    "    to_ascii=False,                # transliterate to closest ASCII representation\n",
    "    lower=True,                    # lowercase text\n",
    "    no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "    no_urls=True,                  # replace all URLs with a special token\n",
    "    no_emails=True,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "    no_numbers=True,               # replace all numbers with a special token\n",
    "    no_digits=True,                # replace all digits with a special token\n",
    "    no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "    no_punct=True,                 # remove punctuations\n",
    "    replace_with_punct=\"\",         # instead of removing punctuations you may replace them\n",
    "    replace_with_url=\"_URL_\",\n",
    "    replace_with_email=\"_EMAIL_\",\n",
    "    replace_with_phone_number=\"_PHONE_\",\n",
    "    replace_with_number=\"_NUMBER_\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"_CUR_\",\n",
    "    lang=\"en\"                    \n",
    "))\n",
    "    #Final cleaning of the 'content' column chunks\n",
    "    chunk_content_column_cleaned = chunk_content_column_cleaned.apply(lambda x: remove_dates_from_content(x))\n",
    "    chunk_content_column_cleaned = chunk_content_column_cleaned.apply(lambda x: remove_bar_from_content(x))\n",
    "    print('Cleaning finished')\n",
    "\n",
    "    #Tokenizing the 'content' column chunks\n",
    "    chunk_content_column_tokenized = chunk_content_column_cleaned.apply(lambda x: nltk.word_tokenize(x))\n",
    "    print('Tokenization finished')\n",
    "    #Removing stopwords from the 'content' column chunks\n",
    "    chunk_content_column_no_stopwords = chunk_content_column_tokenized.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "    #Stemming the 'content' column chunks\n",
    "    chunk_content_column_preprocessed = chunk_content_column_no_stopwords.apply(lambda x:list_stemmer(x))\n",
    "    print('Stemming finished')\n",
    "    preprocessed_content_list.extend(chunk_content_column_preprocessed.tolist())\n",
    "\n",
    "print('Preprocessing finished')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emilo\\AppData\\Local\\Temp\\ipykernel_2468\\4178191706.py:5: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  large_dataset = pd.read_csv('995,000_rows.csv')\n"
     ]
    }
   ],
   "source": [
    "#Converting preprocessed list to pandas series\n",
    "preprocessed_content_series = pd.Series(preprocessed_content_list)\n",
    "\n",
    "#Reading in the original dataset\n",
    "large_dataset = pd.read_csv('995,000_rows.csv')\n",
    "\n",
    "#Altering the 'content' to contain the preprocessed content\n",
    "large_dataset['content'] = preprocessed_content_series\n",
    "\n",
    "#Saving preprocessed dataframe to new csv file\n",
    "large_dataset.to_csv('large_dataset_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emilo\\AppData\\Local\\Temp\\ipykernel_2468\\3759598367.py:10: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('large_dataset_cleaned.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['articl', 'googl', 'ali', 'alfoneh', 'assist', 'compil', 'polit', 'nuclear', 'issu', 'suprem', 'leader', 'tell', 'islam', 'student', 'associ', 'foreign', 'univers', 'conspiraci', 'machin', 'enemi', 'includ', 'scientif', 'apartheid', 'subject', 'nation', 'strengthen', 'uniti', 'peopl', 'head', 'iran', 'nuclear', 'energi', 'agenc', 'condit', 'implement', 'addit', 'protocol', 'reactor', 'come', 'onlin', 'militari', 'admir', 'habiballah', 'sayyari', 'chief', 'islam', 'republ', 'iran', 'navi', 'closur', 'hormuz', 'strait', 'consider', 'upcom', 'war', 'game', 'ad', 'iranianmad', 'submarin', 'leav', 'dock', 'southern', 'iran', 'seyyedyahya', 'rahim', 'safavi', 'irgc', 'head', 'current', 'advisor', 'suprem', 'leader', 'risk', 'attack', 'iran', 'minim', 'iran', 'defens', 'doctrin', 'entail', 'nuclear', 'weapon', 'societi', 'cultur', 'iranian', 'psychologist', 'davar', 'sheikhavandi', 'window', 'shop', 'bring', 'girl', 'boy', 'social', 'danger', 'environ', 'shop', 'mall', 'a', 'prelud', 'decad', 'islam', 'republ', 'minist', 'interior', 'inform', 'public', 'preislam', 'nowruz', 'year', 'holiday', 'iran', 'public', 'health', 'minist', 'health', 'iranianproduc', 'aid', 'medicin', 'regist', 'intern', 'background', 'februari', 'number', 'number', 'iran', 'news', 'round', 'diplomaci', 'return', 'baku', 'speaker', 'iranian', 'parliament', 'gholamali', 'haddad', 'adel', 'inform', 'expans', 'iranianazeri', 'cultur', 'exchang', 'aftab', 'yazd', 'claim', 'iranian', 'pilgrim', 'iraq', 'obtain', 'a', 'visa', 'iraqi', 'embassi', 'tehran', 'turn', 'back', 'iraniraq', 'border', 'theater', 'annapoli', 'a', 'fiasco', 'conven', 'write', 'mouthpiec', 'iranian', 'suprem', 'leader', 'ayatollah', 'ali', 'khamnei', 'kayhan', 'ahmadinejad', 'warn', 'state', 'attend', 'seneg', 'support', 'iran', 'nuclear', 'program', 'ivori', 'coast', 'foreign', 'minist', 'tehran', 'press', 'jamal', 'rahimiyan', 'univers', 'jihad', 'organ', 'appoint', 'chief', 'editor', 'iranian', 'student', 'news', 'agenc', 'isna', 'economi', 'trade', 'photo', 'day']\n"
     ]
    }
   ],
   "source": [
    "# #None of the following code has been run yet!!\n",
    "\n",
    "# large_dataset_cleaned = pd.read_csv('large_dataset_cleaned.csv')\n",
    "# #Splitting data into train_test_val sets:\n",
    "\n",
    "# #Dividing data into features and label (X and y)\n",
    "# y = large_dataset_cleaned['type'] #target\n",
    "\n",
    "# X = large_dataset_cleaned.drop(columns=['type']) #features\n",
    "data = pd.read_csv('large_dataset_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     ['articl', 'googl', 'ali', 'alfoneh', 'assist', 'compil', 'polit', 'nuclear', 'issu', 'suprem', 'leader', 'tell', 'islam', 'student', 'associ', 'f...\n",
      "1     ['cost', 'senat', 'bank', 'committe', 'jp', 'morgan', 'buy', 'cur000000', 'bribe', 'news', 'hedg', 'hour', 'time', 'jami', 'dimon', 'sit', 'senat'...\n",
      "2     ['man', 'awoken', 'numberyear', 'coma', 'commit', 'suicid', 'learn', 'donald', 'trump', 'lead', 'presidenti', 'race', 'fatal', 'error', 'call', 'u...\n",
      "3     ['julia', 'geist', 'ask', 'draw', 'a', 'pictur', 'a', 'comput', 'scientist', 'year', 'numberyearold', 'sketch', 'a', 'businessman', 'wear', 'glass...\n",
      "4     ['number', 'compil', 'studi', 'vaccin', 'danger', 'activist', 'post', 'sep', 'number', 'number', 'shortag', 'research', 'negat', 'effect', 'a', 'w...\n",
      "5     ['spend', 'major', 'wake', 'hour', 'stare', 'content', 'a', 'comput', 'smartphon', 'will', 'ignor', 'ocular', 'havoc', 'blue', 'light', 'electron'...\n",
      "6     ['disclaim', 'general', 'inform', 'a', 'law', 'topic', 'diari', 'constitut', 'legal', 'advic', 'act', 'legal', 'advic', 'advic', 'a', 'skill', 'pr...\n",
      "7     ['a', 'report', 'identifi', 'number', 'epicent', 'worldwid', 'chang', 'climat', 'spark', 'conflict', 'nation', 'secur', 'establish', 'prepar', 'a'...\n",
      "8                                                    ['polic', 'final', 'left', 'campus', 'number', 'civilian', 'lay', 'dead', 'courtyard', 'shot', 'head']\n",
      "9     ['dear', 'reader', 'excit', 'announc', 'voic', 'russia', 'chang', 'move', 'a', 'websit', 'sputnik', 'news', 'agenc', 'radio', 'find', 'latest', 's...\n",
      "10    ['articl', 'googl', 'ali', 'alfoneh', 'assist', 'compil', 'polit', 'nuclear', 'issu', 'suprem', 'leader', 'tell', 'islam', 'student', 'associ', 'f...\n",
      "11    ['republican', 'privat', 'medicar', 'make', 'day', 'sen', 'chuck', 'schumer', 'getti', 'schumer', 'accus', 'gop', 'war', 'senior', 'share', 'faceb...\n",
      "12                                                                 ['your', 'ad', 'blocker', 'whitelist', 'disabl', 'abovetopsecretcom', 'adblock', 'tool']\n",
      "13    ['oppos', 'plan', 'parent', 'hood', 'a', 'reprint', 'stopp', 'essay', 'oppos', 'plan', 'parenthood', 'plan', 'parenthood', 'a', 'benevol', 'organ'...\n",
      "14    ['getti', 'nobl', 'fals', 'widow', 'spider', 'a', 'distinct', 'white', 'mark', 'fals', 'widow', 'spider', 'fearsom', 'fals', 'widow', 'spider', 'm...\n",
      "15    ['san', 'francisco', 'california', 'eff', '\\xadthe', 'electron', 'frontier', 'foundat', 'eff', 'aclu', 'foundat', 'southern', 'california', 'aclu'...\n",
      "16    ['mainstream', 'corpor', 'media', 'visibl', 'die', 'unit', 'state', 'world', 'problem', 'wors', 'antimedia', 'roll', 'project', 'homepag', 'indepe...\n",
      "17    ['baghdad', 'reuter', 'islam', 'state', 'sunni', 'milit', 'group', 'seek', 'establish', 'a', 'caliph', 'part', 'iraq', 'syria', 'releas', 'a', 'vi...\n",
      "18                                                                                                        ['express', 'home', 'daili', 'sunday', 'express']\n",
      "19    ['news', 'governor', 'palin', 'today', 'tweet', 'sarahpalinusa', 'pall', 'terrorist', 'underst', 'case', 'a', 'war', 'women', 'side', 'presid', 'o...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# #Split data into 80% training and 20% test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# #Split the newly created test data equally into validation and test data (10% each of the total dataset)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "print(data['content'][:20])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
