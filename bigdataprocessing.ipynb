{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emilo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra cleaning functions for dates and \"|\" symbol\n",
    "def remove_dates_from_content(content):\n",
    "    '''Function that attempts to substitute dates in a document for the token \"_DATE_\".\n",
    "    If it fails to do so - for example if the content is not convertable to string, it \n",
    "    handles the typeerror exception and doesnt do anything with the content.'''\n",
    "    date_pattern = re.compile(r\"(([0-9]{4}-(0[0-9]|1[0-2])-([0-2][0-9]|[3[01])|[a-z]{,9} [0-9]{1,2}, [0-9]{2,4})|\\b(\\w+\\s)(\\d{2})(th)?,?(\\s\\d{4})\\b)\")\n",
    "    try:\n",
    "        content_without_dates = re.sub(date_pattern, \"_DATE_\", str(content))\n",
    "    except TypeError:\n",
    "        content_without_dates = content\n",
    "    return content_without_dates \n",
    "\n",
    "def remove_bar_from_content(content):\n",
    "    '''Function for removing every occurence of \"|\"'''\n",
    "    content_without_bar = str(content).replace(\"|\", \"\")\n",
    "    return content_without_bar\n",
    "\n",
    "def remove_a_from_content(content):\n",
    "    '''Function for removing every occurence of \"a\"'''\n",
    "    return [word for word in content if word != \"a\"]\n",
    "\n",
    "#Reading in stopwords\n",
    "stopwordsDF = pd.read_csv('stopwords.csv')\n",
    "stopwordsSeries = stopwordsDF.squeeze()\n",
    "stopwords = set(stopwordsSeries)\n",
    "\n",
    "def remove_stopwords(list):\n",
    "     '''Function that returns a list containing a document with the stopwords removed'''\n",
    "     return [word for word in list if word not in stopwords]\n",
    "\n",
    "#Initializing stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def list_stemmer (wordlist): #stemmer hvert ord i en liste\n",
    "    '''Function that stems each word in the given input list and returns this'''\n",
    "    stemmed_list = []\n",
    "    for word in wordlist:\n",
    "        stemmed_list.append(stemmer.stem(word))\n",
    "    return stemmed_list\n",
    "\n",
    "# def vocabulary_size(series):\n",
    "#     '''Function that computes the size of the vocabulary of a corpus'''\n",
    "\n",
    "#     #Initializing empty set to store unique words\n",
    "#     unique_words_in_corpus = set()\n",
    "\n",
    "#     for lst in series:\n",
    "#         #Updates the set of unique words by union with all lists in the corpus\n",
    "#         unique_words_in_corpus.update(lst)\n",
    "    \n",
    "#     #Computing length (sz of vocabulary)\n",
    "#     vocab_sz = len(unique_words_in_corpus)\n",
    "\n",
    "#     return vocab_sz\n",
    "\n",
    "# def most_frequent_n_words(series, n):\n",
    "#     '''Function that return the n most frequent words and their frequencies in a series\n",
    "#     assuming the elements in the series are lists of strings'''\n",
    "#     words = (word for sublist in series for word in sublist)\n",
    "    \n",
    "#     # Calculate word frequencies using Counter\n",
    "#     word_freq = Counter(words)\n",
    "    \n",
    "#     # Select the top 10,000 most common words and frequencies\n",
    "#     most_common_words = word_freq.most_common(n)\n",
    "\n",
    "#     return most_common_words\n",
    "\n",
    "#Jeg har taget foreskellen ml antal ord før processing og efter of så divideret med\n",
    "# antal ord til at starte med for at få reduction raten\n",
    "def reduction_rate(after,before):\n",
    "     '''Computes the reduction rate of the size of the vocabulary\n",
    "     and returns this rounded to 3 decimal points'''\n",
    "     return round((before - after)/before, 3)\n",
    "\n",
    "def word_frequency_plot(counter_dict, title):\n",
    "\n",
    "    # Select the top 10,000 most common words and frequencies\n",
    "    most_common_words = counter_dict.most_common(10000)\n",
    "    words, frequencies = zip(*most_common_words)\n",
    "    \n",
    "    # Creating the plot\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.bar(range(len(frequencies)), frequencies, width=1.0)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Words Ranked by Frequency')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.yscale('log')  # Using logarithmic scale for better visibility of frequencies\n",
    "    \n",
    "    # Removing x-ticks\n",
    "    plt.xticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# def count_occurences_in_content(content, str):\n",
    "#     count = sum(string == str for document in content for string in document)\n",
    "#     return count\n",
    "\n",
    "def update_frequency_counter(frequency_counter, content):\n",
    "    for list in content:\n",
    "        frequency_counter.update(list)\n",
    "    return frequency_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning finished\n",
      "Tokenization finished\n",
      "Stemming finished\n",
      "Cleaning finished\n",
      "Tokenization finished\n",
      "Stemming finished\n",
      "Cleaning finished\n",
      "Tokenization finished\n",
      "Stemming finished\n",
      "Cleaning finished\n",
      "Tokenization finished\n",
      "Stemming finished\n",
      "Preprocessing finished\n"
     ]
    }
   ],
   "source": [
    "# Applying data preprocessing pipeline to 995,000_rows.csv dataset split into chunks for better memory management\n",
    "\n",
    "chunk_size = 124375\n",
    "large_dataset_chunks = pd.read_csv('995,000_rows.csv', low_memory=False, chunksize=chunk_size)\n",
    "\n",
    "#Initializing list to save the processed data\n",
    "preprocessed_content_list = []\n",
    "#Initializing dictionaries to keep track of words and their frequencies\n",
    "#before and after both removing stopwords and stemming the corpus.\n",
    "word_frequencies_tokenized = Counter()\n",
    "word_frequencies_no_stopwords = Counter()\n",
    "word_frequencies_stemmed = Counter()\n",
    "\n",
    "for chunk in large_dataset_chunks:\n",
    "\n",
    "    #Extracting 'content' column chunks and giving dtype string\n",
    "    chunk_content = chunk['content'].astype(str)\n",
    "\n",
    "    #Cleaning the 'content' column chunks\n",
    "    chunk_content = chunk_content.apply(lambda x : clean(x,\n",
    "    fix_unicode=False,             # fix various unicode errors\n",
    "    to_ascii=False,                # transliterate to closest ASCII representation\n",
    "    lower=True,                    # lowercase text\n",
    "    no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "    no_urls=True,                  # replace all URLs with a special token\n",
    "    no_emails=True,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "    no_numbers=True,               # replace all numbers with a special token\n",
    "    no_digits=True,                # replace all digits with a special token\n",
    "    no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "    no_punct=True,                 # remove punctuations\n",
    "    replace_with_punct=\"\",         # instead of removing punctuations you may replace them\n",
    "    replace_with_url=\"_URL_\",\n",
    "    replace_with_email=\"_EMAIL_\",\n",
    "    replace_with_phone_number=\"_PHONE_\",\n",
    "    replace_with_number=\"_NUMBER_\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"_CUR_\",\n",
    "    lang=\"en\"                    \n",
    "))\n",
    "    \n",
    "    #Final cleaning of the 'content' column chunks\n",
    "    chunk_content = chunk_content.apply(lambda x: remove_dates_from_content(x))\n",
    "    chunk_content = chunk_content.apply(lambda x: remove_bar_from_content(x))\n",
    "    print('Cleaning finished')\n",
    "\n",
    "    #Tokenizing the 'content' column chunks\n",
    "    chunk_content = chunk_content.apply(lambda x: nltk.word_tokenize(x))\n",
    "    print('Tokenization finished')\n",
    "\n",
    "    #Updating the \"word_frequency_tokenized\" dictionary\n",
    "    update_frequency_counter(word_frequencies_tokenized, chunk_content)\n",
    "\n",
    "    #Removing stopwords from the 'content' column chunks\n",
    "    chunk_content = chunk_content.apply(lambda x: remove_stopwords(x))\n",
    "    chunk_content = chunk_content.apply(lambda x: remove_a_from_content(x))\n",
    "\n",
    "    #Updating the \"word_frequency_no_stopwords\" dictionary\n",
    "    update_frequency_counter(word_frequencies_no_stopwords, chunk_content)\n",
    "\n",
    "    #Stemming the 'content' column chunks\n",
    "    chunk_content = chunk_content.apply(lambda x:list_stemmer(x))\n",
    "\n",
    "    #Updating the \"word_frequency_stemmed\" dictionary\n",
    "    update_frequency_counter(word_frequencies_stemmed, chunk_content)\n",
    "    \n",
    "    print('Stemming finished')\n",
    "    preprocessed_content_list.extend(chunk_content.tolist())\n",
    "\n",
    "print('Preprocessing finished')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['articl', 'googl', 'ali', 'alfoneh', 'assist', 'compil', 'polit', 'nuclear', 'issu', 'suprem', 'leader', 'tell', 'islam', 'student', 'associ', 'foreign', 'univers', 'conspiraci', 'machin', 'enemi', 'includ', 'scientif', 'apartheid', 'subject', 'nation', 'strengthen', 'uniti', 'peopl', 'head', 'iran', 'nuclear', 'energi', 'agenc', 'condit', 'implement', 'addit', 'protocol', 'reactor', 'come', 'onlin', 'militari', 'admir', 'habiballah', 'sayyari', 'chief', 'islam', 'republ', 'iran', 'navi', 'closur', 'hormuz', 'strait', 'consider', 'upcom', 'war', 'game', 'ad', 'iranianmad', 'submarin', 'leav', 'dock', 'southern', 'iran', 'seyyedyahya', 'rahim', 'safavi', 'irgc', 'head', 'current', 'advisor', 'suprem', 'leader', 'risk', 'attack', 'iran', 'minim', 'iran', 'defens', 'doctrin', 'entail', 'nuclear', 'weapon', 'societi', 'cultur', 'iranian', 'psychologist', 'davar', 'sheikhavandi', 'window', 'shop', 'bring', 'girl', 'boy', 'social', 'danger', 'environ', 'shop', 'mall', 'prelud', 'decad', 'islam', 'republ', 'minist', 'interior', 'inform', 'public', 'preislam', 'nowruz', 'year', 'holiday', 'iran', 'public', 'health', 'minist', 'health', 'iranianproduc', 'aid', 'medicin', 'regist', 'intern', 'background', 'februari', 'number', 'number', 'iran', 'news', 'round', 'diplomaci', 'return', 'baku', 'speaker', 'iranian', 'parliament', 'gholamali', 'haddad', 'adel', 'inform', 'expans', 'iranianazeri', 'cultur', 'exchang', 'aftab', 'yazd', 'claim', 'iranian', 'pilgrim', 'iraq', 'obtain', 'visa', 'iraqi', 'embassi', 'tehran', 'turn', 'back', 'iraniraq', 'border', 'theater', 'annapoli', 'fiasco', 'conven', 'write', 'mouthpiec', 'iranian', 'suprem', 'leader', 'ayatollah', 'ali', 'khamnei', 'kayhan', 'ahmadinejad', 'warn', 'state', 'attend', 'seneg', 'support', 'iran', 'nuclear', 'program', 'ivori', 'coast', 'foreign', 'minist', 'tehran', 'press', 'jamal', 'rahimiyan', 'univers', 'jihad', 'organ', 'appoint', 'chief', 'editor', 'iranian', 'student', 'news', 'agenc', 'isna', 'economi', 'trade', 'photo', 'day'], ['cost', 'senat', 'bank', 'committe', 'jp', 'morgan', 'buy', 'cur000000', 'bribe', 'news', 'hedg', 'hour', 'time', 'jami', 'dimon', 'sit', 'senat', 'bank', 'committe', 'prove', 'smarter', 'call', 'shot', 'great', 'wall', 'streetdc', 'soap', 'opera', 'purchas', 'room', 'full', 'senat', 'mention', 'script', 'today', 'hear', 'jp', 'morgan', 'top', 'tabl', 'compil', 'opensecret', 'data', 'cost', 'jp', 'morgan', 'cur0', 'million', 'cur00000000', 'precis', 'lifetim', 'campaign', 'contribut', 'buy', 'precis', 'senat', 'bank', 'committe', 'fun', 'chairman', 'tim', 'johnson', 'sd', 'rank', 'member', 'richard', 'shelbi', 'al', 'jp', 'morgan', 'top', 'biggest', 'campaign', 'contributor', 'continu', 'read', 'zerohedgecom', 'read', 'financi', 'surviv', 'network', 'sourc']]\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_content_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_content_series = pd.Series(preprocessed_content_list)\n",
    "\n",
    "preprocessed_content_series.to_pickle('content_preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset.to_pickle('995,000_rows_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of URL's in content:  263039\n",
      "Count of dates in content:  44720\n",
      "Count of numbers in content:  6209052\n"
     ]
    }
   ],
   "source": [
    "url_count = word_frequencies_tokenized['url']\n",
    "date_count = word_frequencies_tokenized['date']\n",
    "number_count = word_frequencies_tokenized['number']\n",
    "\n",
    "print(\"Count of URL's in content: \", url_count)\n",
    "print(\"Count of dates in content: \", date_count)\n",
    "print(\"Count of numbers in content: \", number_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most common words in tokenized data:  [('the', 26149872), ('to', 12608807), ('of', 12462264), ('and', 11434235), ('a', 9847694), ('in', 8649507), ('number', 6209052), ('that', 5463746), ('is', 5063094), ('for', 4369621), ('on', 3487089), ('it', 2991866), ('with', 2827783), ('as', 2795151), ('was', 2443526), ('are', 2437801), ('by', 2329672), ('this', 2275935), ('not', 2188344), ('at', 2184696), ('be', 2165001), ('have', 2095758), ('i', 2047978), ('from', 2038249), ('he', 2023919), ('you', 1986005), ('an', 1717358), ('has', 1706949), ('but', 1638333), ('his', 1629409), ('they', 1588561), ('or', 1510196), ('we', 1483259), ('said', 1410689), ('its', 1364965), ('will', 1330845), ('who', 1326790), ('their', 1268986), ('more', 1156366), ('all', 1156080), ('about', 1105097), ('new', 1057199), ('one', 1055274), ('were', 1026236), ('which', 1021229), ('if', 995090), ('us', 976950), ('can', 967774), ('would', 958311), ('been', 924626), ('up', 923144), ('had', 915040), ('what', 896012), ('people', 875692), ('when', 854430), ('so', 833707), ('our', 829098), ('out', 821801), ('there', 812853), ('no', 774706), ('your', 701062), ('also', 690893), ('like', 684359), ('her', 679403), ('than', 676739), ('do', 663845), ('other', 651714), ('after', 650393), ('some', 640633), ('she', 632070), ('just', 620315), ('time', 603041), ('them', 597390), ('into', 595040), ('my', 594498), ('now', 594121), ('over', 590970), ('only', 571121), ('mr', 564343), ('even', 535029), ('how', 510665), ('years', 502358), ('first', 500351), ('most', 499005), ('because', 498890), ('two', 494037), ('these', 493924), ('news', 488510), ('could', 487575), ('many', 486871), ('state', 471618), ('president', 465967), ('any', 459801), ('government', 446548), ('may', 444699), ('those', 441641), ('before', 436837), ('states', 421717), ('get', 421102), ('last', 421047)]\n",
      "100 most common words in data after removing stopwords:  [('number', 6209052), ('people', 875692), ('time', 603041), ('mr', 564343), ('years', 502358), ('news', 488510), ('state', 471618), ('president', 465967), ('government', 446548), ('states', 421717), ('year', 415090), ('world', 409921), ('trump', 403790), ('obama', 401623), ('story', 373033), ('make', 366969), ('numbernumber', 366486), ('back', 361499), ('public', 360204), ('american', 357696), ('dont', 350306), ('york', 345398), ('day', 333682), ('war', 319012), ('made', 310717), ('united', 310199), ('continue', 305706), ('house', 291544), ('system', 285188), ('times', 282619), ('health', 281982), ('good', 278031), ('work', 278015), ('percent', 265328), ('url', 263039), ('national', 258788), ('main', 257149), ('reading', 251699), ('country', 249826), ('life', 249478), ('political', 247754), ('iran', 243853), ('law', 242576), ('white', 242274), ('including', 240223), ('part', 235665), ('million', 231466), ('told', 229788), ('military', 229094), ('nuclear', 228868), ('media', 225458), ('money', 220430), ('long', 217809), ('recs', 214966), ('police', 214105), ('america', 209080), ('week', 208522), ('fact', 208153), ('support', 207498), ('city', 206562), ('group', 205801), ('home', 205558), ('party', 203647), ('end', 203467), ('today', 203464), ('federal', 201196), ('im', 200382), ('called', 199983), ('power', 199206), ('sign', 197375), ('email', 194035), ('foreign', 193757), ('found', 193223), ('big', 192118), ('security', 191838), ('report', 191135), ('campaign', 189239), ('great', 187597), ('free', 187448), ('things', 187253), ('iranian', 186180), ('man', 184997), ('high', 184564), ('show', 183457), ('leader', 182689), ('children', 180868), ('cur00', 180640), ('information', 180194), ('family', 179730), ('minister', 179634), ('article', 178007), ('case', 177299), ('place', 176388), ('в', 175906), ('left', 173988), ('order', 173223), ('republican', 172971), ('youre', 170373), ('women', 170363), ('bill', 170267)]\n",
      "100 most common words in stemmed data:  [('number', 6300952), ('state', 945493), ('year', 920232), ('peopl', 918898), ('time', 900679), ('make', 622913), ('mr', 564343), ('report', 543917), ('presid', 538202), ('govern', 537648), ('work', 523812), ('american', 519339), ('nation', 509511), ('trump', 498223), ('day', 495913), ('news', 488514), ('obama', 486131), ('world', 460193), ('continu', 453455), ('stori', 449160), ('read', 437453), ('call', 437307), ('public', 414193), ('includ', 413093), ('countri', 401836), ('back', 399710), ('support', 383657), ('polit', 376538), ('numbernumb', 366668), ('unit', 364569), ('iran', 356072), ('war', 353655), ('hous', 353216), ('york', 351092), ('show', 350740), ('dont', 350590), ('system', 335325), ('week', 327237), ('live', 323988), ('group', 321502), ('thing', 321447), ('law', 317289), ('made', 310726), ('good', 303494), ('democrat', 300335), ('compani', 298430), ('issu', 296205), ('million', 296097), ('republican', 295895), ('part', 290145), ('inform', 289816), ('sign', 287534), ('power', 284409), ('point', 284330), ('offic', 284113), ('health', 282817), ('end', 279664), ('plan', 277929), ('leader', 273870), ('forc', 270707), ('servic', 269603), ('citi', 268271), ('attack', 266586), ('main', 266557), ('percent', 265881), ('url', 263306), ('parti', 263159), ('vote', 262835), ('offici', 262380), ('america', 262376), ('secur', 262298), ('white', 258071), ('elect', 254587), ('life', 252509), ('chang', 251986), ('famili', 251153), ('univers', 248272), ('person', 247095), ('fact', 246425), ('month', 245928), ('case', 244266), ('market', 243550), ('start', 242730), ('home', 241720), ('play', 236689), ('offer', 236489), ('today', 235898), ('recent', 235239), ('product', 234054), ('militari', 232808), ('find', 232426), ('school', 231801), ('talk', 230696), ('media', 230279), ('told', 229792), ('email', 229434), ('nuclear', 229064), ('ad', 228583), ('high', 228238), ('order', 227912)]\n"
     ]
    }
   ],
   "source": [
    "#100 most common words in tokenized data\n",
    "most_common_tokenized = word_frequencies_tokenized.most_common(100)\n",
    "\n",
    "#100 most common words in data after removing stopwords\n",
    "most_common_no_stopwords = word_frequencies_no_stopwords.most_common(100)\n",
    "\n",
    "#100 most common words in stemmed data\n",
    "most_common_stemmed = word_frequencies_stemmed.most_common(100)\n",
    "\n",
    "print('100 most common words in tokenized data: ', most_common_tokenized)\n",
    "print('100 most common words in data after removing stopwords: ', most_common_no_stopwords)\n",
    "print('100 most common words in stemmed data: ', most_common_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_frequency_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Plotting the frequency of the 10000 most frequent words - tokenized:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mword_frequency_plot\u001b[49m(word_frequencies_tokenized, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokenized\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Plotting the frequency of the 10000 most frequent words - no stopwords:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m word_frequency_plot(word_frequencies_no_stopwords, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_frequency_plot' is not defined"
     ]
    }
   ],
   "source": [
    "#Plotting the frequency of the 10000 most frequent words - tokenized:\n",
    "word_frequency_plot(word_frequencies_tokenized, 'Tokenized')\n",
    "#Plotting the frequency of the 10000 most frequent words - no stopwords:\n",
    "word_frequency_plot(word_frequencies_no_stopwords, 'No stopwords')\n",
    "#Plotting the frequency of the 10000 most frequent words - stemmed:\n",
    "word_frequency_plot(word_frequencies_stemmed, 'Stemmed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
