{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, name,test_vec):\n",
    "    print(f\"evaluating {name}\")\n",
    "    y_pred = model.predict(test_vec)\n",
    "    print(\"accuracy:\", accuracy_score(y_pred,y_test))\n",
    "    print(\"F1:\", f1_score(y_pred,y_test))\n",
    "    precision = precision_score(y_val, y_test)\n",
    "    recall = recall_score(y_val, y_test)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    cm = confusion_matrix(y_true=y_test,y_pred=y_test)\n",
    "    fig, ax = plt.subplots(figsize=(3,3))\n",
    "    ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='medium')\n",
    "    plt.xlabel('Predicted', fontsize=8)\n",
    "    plt.ylabel('Actual', fontsize=8)\n",
    "    plt.title(f'Confusion Matrix, {name}', fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_len_test = pd.DataFrame(X_test.apply(lambda x: len(x))) \n",
    "evaluate(baseline1, \"baseline: length of articles\",X_len_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fact_test = pd.DataFrame(X_test.apply(lambda x: word_count_reg(x,\"fact\")))\n",
    "evaluate(baseline2, 'baseline: # of \"facts\"',X_fact_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluering med liar-datas√¶t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar = pd.read_pickle(\"labeled_liar_statements_preprocessed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = set([\"half-true\",\"mostly-true\",\"barely-true\",\"true\"])\n",
    "false = set([\"false\",\"pants-fire\"])\n",
    "def get_bin_y_liar(x):\n",
    "    if x in true:\n",
    "        return 0\n",
    "    if x in false:\n",
    "        return 1\n",
    "    \n",
    "liar[\"Label\"] = liar[\"Label\"].apply(lambda x: get_bin_y_liar(x))\n",
    "print(len(liar))\n",
    "liar = liar[liar[\"Label\"]==1 or liar[\"Label\"]==0] #fjern nans\n",
    "print(len(liar))\n",
    "y_test = liar[\"Label\"]\n",
    "X_test = liar[\"Statement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline 1\n",
    "X_test_baseline1=X_test.apply(lambda x: len(x))\n",
    "X_test_baseline1.describe() #get length statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_baseline2 = X_test.apply(lambda x:word_count_reg(x,\"fact\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(baseline1, \"baseline model\", X_test_baseline1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(baseline2, \"baseline model\", X_test_baseline2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
