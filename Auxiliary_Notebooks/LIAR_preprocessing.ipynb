{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emilo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "import functions as funs\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in stopwords\n",
    "stopwordsDF = pd.read_csv('stopwords.csv')\n",
    "stopwordsSeries = stopwordsDF.squeeze()\n",
    "stopwords = set(stopwordsSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_train_data = pd.read_csv('data/LIAR/train.tsv', sep='\\t', header=None)\n",
    "liar_test_data = pd.read_csv('data/LIAR/test.tsv', sep='\\t', header=None)\n",
    "liar_valid_data = pd.read_csv('data/LIAR/valid.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_statements = pd.Series(liar_train_data.iloc[:, 2])\n",
    "test_statements = pd.Series(liar_test_data.iloc[:, 2])\n",
    "valid_statements = pd.Series(liar_valid_data.iloc[:, 2])\n",
    "\n",
    "train_labels = pd.Series(liar_train_data.iloc[:, 1])\n",
    "test_labels = pd.Series(liar_test_data.iloc[:, 1])\n",
    "valid_labels = pd.Series(liar_valid_data.iloc[:, 1])\n",
    "\n",
    "all_statements = pd.concat([train_statements, test_statements, valid_statements], axis=0, ignore_index=True)\n",
    "all_labels = pd.concat([train_labels, test_labels, valid_labels], axis=0, ignore_index=True)\n",
    "\n",
    "labeled_statements = pd.concat([all_statements, all_labels], axis=1)\n",
    "labeled_statements.columns = ['Statement', 'Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing finished\n"
     ]
    }
   ],
   "source": [
    "#Preprocess pipeline\n",
    "#Extracting 'statement' column\n",
    "statements = labeled_statements['Statement']\n",
    "\n",
    "#Cleaning the 'statement' column\n",
    "statements = statements.apply(lambda x : clean(x,\n",
    "fix_unicode=False,             # fix various unicode errors\n",
    "to_ascii=False,                # transliterate to closest ASCII representation\n",
    "lower=True,                    # lowercase text\n",
    "no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "no_urls=True,                  # replace all URLs with a special token\n",
    "no_emails=True,                # replace all email addresses with a special token\n",
    "no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "no_numbers=True,               # replace all numbers with a special token\n",
    "no_digits=True,                # replace all digits with a special token\n",
    "no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "no_punct=True,                 # remove punctuations\n",
    "replace_with_punct=\"\",         # instead of removing punctuations you may replace them\n",
    "replace_with_url=\"_URL_\",\n",
    "replace_with_email=\"_EMAIL_\",\n",
    "replace_with_phone_number=\"_PHONE_\",\n",
    "replace_with_number=\"_NUMBER_\",\n",
    "replace_with_digit=\"0\",\n",
    "replace_with_currency_symbol=\"_CUR_\",\n",
    "lang=\"en\"                    \n",
    "))\n",
    "\n",
    "#Final cleaning of the 'statement' column\n",
    "statements = statements.apply(lambda x: funs.remove_dates_from_content(x))\n",
    "statements = statements.apply(lambda x: funs.remove_bar_from_content(x))\n",
    "\n",
    "#Tokenizing the 'content' column chunks\n",
    "statements = statements.apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "#Removing stopwords from the 'content' column chunks\n",
    "statements = statements.apply(lambda x: funs.remove_stopwords(x, stopwords))\n",
    "statements = statements.apply(lambda x: funs.remove_a_from_content(x))\n",
    "\n",
    "#Stemming the 'content' column chunks\n",
    "statements = statements.apply(lambda x:funs.list_stemmer(x))\n",
    "\n",
    "print('Preprocessing finished')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_statements_preprocessed = pd.concat([statements, labeled_statements['Label']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment below to save file to pickle\n",
    "# labeled_statements_preprocessed.to_pickle('labeled_liar_statements_preprocessed.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
